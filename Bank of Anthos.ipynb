{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86075d54-da69-4199-86e1-6f6a45826c26",
   "metadata": {},
   "source": [
    "# Welcome to our playground!\n",
    "\n",
    "It's wonderful to be here\n",
    "It's certainly a thrill\n",
    "You're such a lovely audience\n",
    "We'd like to take you home with us\n",
    "We'd love to take you home\n",
    "\n",
    "## Now let's first see if we can connect to the bank!\n",
    "\n",
    "To do so we need to install some packages and set up the sql extension for Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef1545-a6a8-4a67-ba41-a0355fce877e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installing some python packages to talk SQLd\n",
    "! pip install ipython-sql\n",
    "! pip install sqlalchemy\n",
    "! pip install psycopg2-binary\n",
    "\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = 'europe-west4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222875d-741a-48cd-a155-c64d9fda03e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can jump right into it now - adjust the following cell with the password you created for the postgres user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41893fa4-08cd-4759-98ed-b3caf854d188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql postgresql://postgres:kcg6a2jg@192.168.142.72:5432/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf227e-0c87-465c-8635-3b813620a273",
   "metadata": {},
   "source": [
    "No news is good news! But to be sure, let's run a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c922c-f119-4200-9b2f-a6405e4ed7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM pg_catalog.pg_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abac14c-8662-48e1-88c1-1ff850a009c6",
   "metadata": {},
   "source": [
    "If all went according to plan you will see a table with the results of the SQL query. Feel free to play around with this and query other stuff if you want!\n",
    "\n",
    "## So we can connect to the data, but where is the AI goodness? \n",
    "\n",
    "Right here! But we will start with something simple - let's see if we can predict the next transaction for a certain user! Let's make a dataset out of the transactions database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a17d7a-816b-43d6-9078-99b7e2c6261d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SQLAlchemy & Pandas\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "# Define the engine to use\n",
    "engine = create_engine(\"postgresql://postgres:kcg6a2jg@192.168.142.72:5432/transactions-db\")\n",
    "table_name = 'transactions'\n",
    "\n",
    "# Capture the table into a dataframe!\n",
    "\n",
    "table_df = pd.read_sql_table(\n",
    "    table_name,\n",
    "    con=engine\n",
    ")\n",
    "# And store the dataframe as a csv file\n",
    "\n",
    "table_df.to_csv(\"all_transactions.csv\")\n",
    "\n",
    "# Split the data in data to train and data the model and data to train the model\n",
    "\n",
    "test_df = table_df.iloc[:10,:]\n",
    "train_df = table_df.iloc[10:,:]\n",
    "\n",
    "test_df.to_csv(\"test_transactions.csv\")\n",
    "train_df.to_csv(\"train_transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb71c5c-50e3-4f3e-8589-fcd9ffcf4548",
   "metadata": {},
   "source": [
    "## AutoGluon\n",
    "\n",
    "Like we mentioned we are starting with something small, using Autogluon. Autogluon will allow you to easily train a model based on a small amount of data (we have only 66 transactions unless you went crazy and added a lot more previously..)\n",
    "\n",
    "We've split the transactions from our table into test and training data and will now start to train a model! The model we are training will be used to predict how much money (amount) will be moved around in the next transaction,\n",
    "based on the other characteristics of the transaction. Isn't that exciting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94edd-c15e-4cfb-9c59-50b6679dcf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install and Import autogluon\n",
    "! pip install autogluon\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset('train_transactions.csv')\n",
    "predictor = TabularPredictor(label='amount').fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c4240-a73a-4b10-9a0e-af87299c4b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional - evaluate the model\n",
    "test_data = TabularDataset('test_transactions.csv')\n",
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1770-59a9-4f55-83ba-cf46d28b18f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we we have a model and evaluated it (not the most amazing stats I know...) we can do a prediction. We'll use the test data, but feel free to create your own test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dc45a-289f-4e1b-ba03-9d1aed80eed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_data.drop(columns=['amount']))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508354e-202c-4180-aac5-d5461a9935d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well it made some predictions, but if you compare the values it predicted to the original in the test data well..kinda meh. Let's see if we can improve this by doing something more fancy!\n",
    "\n",
    "## LLMs\n",
    "\n",
    "By now we hope we don't have to explain to you what an LLM is - but if that is the case we'd suggest you'd ask ChatGPT! \n",
    "\n",
    "Normally you'd turn to a LLM for anything related to language, however they aren't half bad at working with numbers and structured data either.\n",
    "\n",
    "### Chatting with Bison. Or is it Bisons..\n",
    "\n",
    "Let's set up a connection to our PALM2 text model - called bison-text, where bison says something about the size of the model. PALM2 is the name of the model (like GPT 1/2/3/4) that powers both Vertex AI search and conversations and for instance Bard.\n",
    "\n",
    "Because we want to know things about our transactions we will use the transactions file as the context when asking the LLM questions. This way it will know how to react and answer. To make this quick and efficient we won't just copy paste the contents of the file into a chat,\n",
    "we will use something called embeddings - this turns chunks of text into mathemetical representations which can be used to really quickly find text similar to our question - and we will send that chunk of text to the LLM.\n",
    "\n",
    "Sounds hocus pocus we know...wait till you see all the code we need :-P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146d39e-ff59-4280-9629-b3a5cdefe009",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Stuff we need to import\n",
    "# \n",
    "! pip install chromadb\n",
    "import time\n",
    "import io\n",
    "from typing import List\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269005d-762a-404b-ba4a-30ec6c48ca74",
   "metadata": {},
   "source": [
    "So now we have imported a bunch of stuff, but we aren't there yet, we need to create a little dirty work around - please just pretend you didn't see this ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3a5ac5-6064-4dbd-bbb7-656d69eab5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "#\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1aef05-5b09-4285-835c-7ed31e0da7f0",
   "metadata": {},
   "source": [
    "Now we are actually going to define the LLM and the Embedding service. Remember, we need to embeddings to feed the LLM our transactions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdab1ff9-be76-4eff-a7af-95eb57958180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "# \n",
    "#  I increased the temp a little, haven't experimented with top k and p to much\n",
    "#\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@002\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 60\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    "    max_output_tokens=1024 # I've changed the default to allow for more output token in the embeddings (default is like 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46f3029-326d-4e8d-8d99-893f4446f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now where would the fun be if we could just import the csv file...we load it and add each row as a separate document\n",
    "loader = CSVLoader(file_path='train_transactions.csv', encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32634e53-f29f-4480-a9bc-3bc75ae55749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the rows in a local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "# This takes a lotta lottta lotta time\n",
    "db = Chroma.from_documents(data, embeddings, persist_directory = \"index_bank\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d9c6f05-d340-436b-a11e-82682583abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max k as a search arguments gives us some room to experiment what works best when using embeddings. \n",
    "#\n",
    "#\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b34b4fae-e3d8-40f9-a313-949acbacc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "#\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd9f27-1d9c-4f07-95fb-497aa1035c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm sure I haven't mastered the art of prompt engineering just yet, but I like this prompt for now. I only replace the question\n",
    "# at the end and pick qa1/2/3 \n",
    "query=\"Based on the context can you predict the next transaction?\"\n",
    "\n",
    "result = qa({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d3e3e-bf8a-43a6-9726-3d9810e7c59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
