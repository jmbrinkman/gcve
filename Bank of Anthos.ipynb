{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86075d54-da69-4199-86e1-6f6a45826c26",
   "metadata": {},
   "source": [
    "# Welcome to our playground!\n",
    "\n",
    "It's wonderful to be here\n",
    "It's certainly a thrill\n",
    "You're such a lovely audience\n",
    "We'd like to take you home with us\n",
    "We'd love to take you home!!\n",
    "\n",
    "## Now let's first see if we can connect to the bank!\n",
    "\n",
    "To do so we need to install some packages and set up the sql extension for Jupyter. We will also set some variables required by the code. \n",
    "\n",
    "***Nota Bene - This might take some time and there will be no output! - > once a number appears in between the brackets [] the command is done!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ef1545-a6a8-4a67-ba41-a0355fce877e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installing some python packages to talk SQL\n",
    "! pip install -q ipython-sql sqlalchemy psycopg2-binary -q\n",
    "\n",
    "import requests\n",
    "\n",
    "project_id= requests.get(\"http://metadata/computeMetadata/v1/project/project-id\", headers={'Metadata-Flavor': 'Google'}).text\n",
    "full_zone_string = requests.get(\"http://metadata/computeMetadata/v1/instance/zone\", headers={'Metadata-Flavor': 'Google'}).text\n",
    "zone_name = full_zone_string.split(\"/\")[3]\n",
    "region = zone_name[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222875d-741a-48cd-a155-c64d9fda03e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can jump right into it now - adjust the following cell with the password you created for the postgres user!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41893fa4-08cd-4759-98ed-b3caf854d188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql postgresql://postgres:kcg6a2jg@192.168.142.72:5432/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf227e-0c87-465c-8635-3b813620a273",
   "metadata": {},
   "source": [
    "No news is good news! But to be sure, let's run a query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c922c-f119-4200-9b2f-a6405e4ed7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM pg_catalog.pg_user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abac14c-8662-48e1-88c1-1ff850a009c6",
   "metadata": {},
   "source": [
    "If all went according to plan you will see a table with the results of the SQL query. Feel free to play around with this and query other stuff if you want!\n",
    "\n",
    "## So we can connect to the data, but where is the AI goodness? \n",
    "\n",
    "Right here! But we will start with something simple - let's see if we can predict the next transaction for a certain user! Let's make a dataset out of the transactions database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a17d7a-816b-43d6-9078-99b7e2c6261d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import SQLAlchemy & Pandas\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "# Define the engine to use\n",
    "engine = create_engine(\"postgresql://postgres:kcg6a2jg@192.168.142.72:5432/transactions-db\")\n",
    "table_name = 'transactions'\n",
    "\n",
    "# Capture the table into a dataframe!\n",
    "\n",
    "table_df = pd.read_sql_table(\n",
    "    table_name,\n",
    "    con=engine\n",
    ")\n",
    "# And store the dataframe as a csv file\n",
    "\n",
    "table_df.rename(columns={'from_acct':'from account','to_acct':'to account'}, inplace=True)\n",
    "table_df.to_csv(\"all_transactions.csv\")\n",
    "\n",
    "# Split the data in data to train and data the model and data to train the model\n",
    "\n",
    "test_df = table_df.iloc[:10,:]\n",
    "train_df = table_df.iloc[10:,:]\n",
    "\n",
    "test_df.to_csv(\"test_transactions.csv\")\n",
    "train_df.to_csv(\"train_transactions.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb71c5c-50e3-4f3e-8589-fcd9ffcf4548",
   "metadata": {},
   "source": [
    "## AutoGluon\n",
    "\n",
    "Like we mentioned we are starting with something small, using Autogluon. Autogluon will allow you to easily train a model based on a small amount of data (we have only 66 transactions unless you went crazy and added a lot more previously..)\n",
    "\n",
    "Let's first install AutoGluon.\n",
    "\n",
    "***Nota Bene - This might take some time and there will be no output! - > once a number appears in between the brackets [] the command is done!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d94edd-c15e-4cfb-9c59-50b6679dcf92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install and Import autogluon\n",
    "! pip install autogluon -q\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b386b0e6-b491-44b6-9879-58faf1b16705",
   "metadata": {},
   "source": [
    "We've split the transactions from our table into test and training data and will now start to train a model! The model we are training will be used to predict how much money (amount) will be moved around in the next transaction,\n",
    "based on the other characteristics of the transaction. Isn't that exciting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6899673-f800-46f8-a0d6-dd82e53ba3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset('train_transactions.csv')\n",
    "predictor = TabularPredictor(label='amount').fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c4240-a73a-4b10-9a0e-af87299c4b6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional - evaluate the model\n",
    "test_data = TabularDataset('test_transactions.csv')\n",
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f1770-59a9-4f55-83ba-cf46d28b18f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we we have a model and evaluated it (not the most amazing stats I know...) we can do a prediction. We'll use the test data, but feel free to create your own test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dc45a-289f-4e1b-ba03-9d1aed80eed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(test_data.drop(columns=['amount']))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d508354e-202c-4180-aac5-d5461a9935d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well it made some predictions, but if you compare the values it predicted to the original in the test data well..kinda meh. Let's see if we can improve this by doing something more fancy!\n",
    "\n",
    "## LLMs\n",
    "\n",
    "By now we hope we don't have to explain to you what an LLM is - but if that is the case we'd suggest you'd ask ChatGPT! \n",
    "\n",
    "Normally you'd turn to a LLM for anything related to language, however they aren't half bad at working with numbers and structured data either.\n",
    "\n",
    "### Chatting with Bison. Or is it Bisons..\n",
    "\n",
    "Let's set up a connection to our PALM2 text model - called bison-text, where bison says something about the size of the model. PALM2 is the name of the model (like GPT 1/2/3/4) that powers both Vertex AI search and conversations and for instance Bard.\n",
    "\n",
    "Because we want to know things about our transactions we will use the transactions file as the context when asking the LLM questions. This way it will know how to react and answer.\n",
    "\n",
    "***Nota Bene - This might take some time and there will be no output! - > once a number appears in between the brackets [] the command is done!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6146d39e-ff59-4280-9629-b3a5cdefe009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Stuff we need to import\n",
    "# \n",
    "! pip install langchain -q\n",
    "import time\n",
    "import io\n",
    "from typing import List\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46f3029-326d-4e8d-8d99-893f4446f157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now where would the fun be if we could not just import the csv file...we load it and add each row as a separate document (something for the LLM to read!)\n",
    "loader = CSVLoader(file_path='all_transactions.csv', encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fe2cc-994e-42c7-b07d-88440b764e8b",
   "metadata": {},
   "source": [
    "Now below we will define how to connect to the Google LLM. **This will produce no output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b06b8873-f971-4153-9e7d-717d824cc9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "\n",
    "parameters = {\n",
    "\"temperature\": 1,\n",
    "\"max_output_tokens\": 256,\n",
    "\"top_p\": 0.8,\n",
    "\"top_k\": 40\n",
    "}\n",
    "   \n",
    "def predict_text(prompt, **parameters):\n",
    "    vertexai.init(project=project_id, location=region)\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison-32k\")\n",
    "    prompt_response = model.predict(prompt,**parameters)\n",
    "    return prompt_response.text\n",
    "\n",
    "command = \"below are some rows with example transactions predict the amount and the to for the next transcation\"\n",
    "text = data\n",
    "prompt = \"\"\"\n",
    "{command}\n",
    "{text}\n",
    "\"\"\".format(text=text, command=command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56314b58-004a-4c0a-ad9b-cefc69765ee3",
   "metadata": {},
   "source": [
    "### The Season Finale\n",
    "\n",
    "And the moment has arrived, run the following cells to make a prediction! If you have the time, feel free to experiment with the setting the cell above this one, some ideas:\n",
    "\n",
    "- See what changing top_p and top_k into other values do\n",
    "- Change the commmand, maybe you can enable it to make a better prediction? Or completly change the prompt and let it do something else, like create more test data?\n",
    "- Maybe you can ask it to explain why it made the prediction?\n",
    "- Always be nice to your LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8dab6-5b76-40df-87b5-d2c6dbaa4c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(predict_text(prompt))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m115"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
